{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258fff3e",
   "metadata": {},
   "source": [
    "## LLM-as-a-Judge evaluation \n",
    "\n",
    "In this notebook, we test some approaches of evaluating responses Kai generates for different migration scenarios using an LLM as a judge.\n",
    "\n",
    "The responses are generated by running Kai manually in vscode with different models. The diff files of the responses are stored as artifacts for simplicity.\n",
    "\n",
    "#### Goal\n",
    "\n",
    "Goal of this exercise is to:\n",
    "- verify & finalize the evaluation metrics\n",
    "- finalize the evaluation prompts\n",
    "\n",
    "### Process\n",
    "\n",
    "Run Tools  -->  Run Kai (manual, responses collected)  -->  Run Tools  -->  Evaluate\n",
    "\n",
    "#### Running tools\n",
    "\n",
    "In this notebook, we run following tools before and after applying fixes from Kai:\n",
    "- `mvn compile`\n",
    "- `mvn test` (only if tests are present for a given test case)\n",
    "- `analyze` (kantra used for analysis)\n",
    "\n",
    "#### Evaluate\n",
    "\n",
    "For evaluating, three metrics are calculated:\n",
    "\n",
    "1. Completeness (C): Measures whether the issue is completely resolved\n",
    "2. Functional Parity (F): Measures whether existing functionality is maintained\n",
    "3. Knock-on Effort (E): Measures how much effort is needed to address new issues caused by the fix\n",
    "\n",
    "The total score is normalized for each metric and a final weighted score is produced between 0-10:\n",
    "\n",
    "```\n",
    "Final Score = 10 * (0.5 * C + 0.3 * F + 0.2 * E)\n",
    "```\n",
    "\n",
    "#### Pre-requisites\n",
    "\n",
    "- Create a virtualenv using Jupyter for running cells in this notebook. Use [requirements.txt](../requirements.txt) to install dependencies needed. \n",
    "- Copy the .env.sample file to .env. Select the model you want to use for evaluation - Only Bedrock and ChatOpenAI are supported. Add your LLM key for the model you want to use. Once setup, run the following cell to load .env file.\n",
    "- Make sure you have _java_ and _mvn_ installed.\n",
    "- Download the latest _kantra_ binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad69a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec81130",
   "metadata": {},
   "source": [
    "Before we begin, we write some common code in the following cell which we will need later on. Run this cell before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0849885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some common functions we will need later on for the evaluation\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import subprocess\n",
    "from git import Repo\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "test_data_path = Path(\"test-data\").absolute()\n",
    "apps_repo_path = (test_data_path / \"apps\").absolute()\n",
    "artifacts_path = (test_data_path / \"artifacts\").absolute()\n",
    "apps_repo_path.mkdir(parents=True, exist_ok=True)\n",
    "artifacts_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def clone_repo(url: str, branch: str, path: Path):\n",
    "    try:\n",
    "        Repo.clone_from(url, depth=1, single_branch=True, branch=branch, to_path=path)\n",
    "    except Exception as e:\n",
    "        if \"already exists\" not in str(e):\n",
    "            print(\"fatal error cloning repo\")\n",
    "            sys.exit(1)\n",
    "\n",
    "def get_model():\n",
    "    provider = os.getenv(\"model_provider\")\n",
    "    model_id = os.getenv(\"model_id\")\n",
    "    if not model_id or not provider:\n",
    "        raise ValueError(\"model_id and/or model_provider are not set\")\n",
    "    match provider:\n",
    "        case \"chatbedrock\":\n",
    "            key_id = os.getenv(\"aws_access_key_id\")\n",
    "            access_key = os.getenv(\"aws_secret_access_key\")\n",
    "            region = os.getenv(\"region\")\n",
    "            if not region or not access_key or not key_id:\n",
    "                raise ValueError(\"aws_region and/or aws_secret_access_key and/or aws_access_key_id is not set\")\n",
    "            return ChatBedrockConverse(\n",
    "                model_id=model_id,\n",
    "                aws_access_key_id=key_id,\n",
    "                aws_secret_access_key=access_key,\n",
    "                region_name=region,\n",
    "            )\n",
    "        case \"chatopenai\":\n",
    "            api_key = os.getenv(\"OPEANAI_API_KEY\") or os.getenv(\"api_key\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"OPEANAI_API_KEY or api_key is not set\")\n",
    "            return ChatOpenAI(model=model_id, api_key=api_key)\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid model provider: {provider}\")\n",
    "\n",
    "def parse_yaml(path: Path): \n",
    "    with open(path, \"r\") as f: return yaml.safe_load(f)\n",
    "\n",
    "def clone_app(app_path: Path) -> Path:\n",
    "    parsed = parse_yaml(app_path)\n",
    "    repo_url = parsed[\"source_code\"][\"git\"][\"url\"]\n",
    "    branch = parsed[\"source_code\"][\"git\"][\"branch\"]\n",
    "    path = Path(\"test-data\") / \"apps\" / parsed[\"name\"]\n",
    "    clone_repo(repo_url, branch, path)\n",
    "    return path.absolute()\n",
    "\n",
    "def get_test_selectors_from_tc(tc_path: str):\n",
    "    parsed = parse_yaml(tc_path)\n",
    "    return parsed[\"testSelectors\"]\n",
    "\n",
    "def run_command(cmd: list[str], stdout_path: Path, stderr_path: Path, cwd: str):\n",
    "    pwd = os.getcwd()\n",
    "    os.chdir(cwd)\n",
    "    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    with open(stdout_path, \"w\") as f:\n",
    "        f.write(result.stdout)\n",
    "    with open(stderr_path, \"w\") as f:\n",
    "        f.write(result.stderr)\n",
    "    os.chdir(pwd)\n",
    "\n",
    "\n",
    "def run_mvn(tc_name: str, app_path: Path, sub_dir: Path = Path(\"\")):\n",
    "    output_dir = (artifacts_path / tc_name / sub_dir / \"mvn\").absolute()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stdout_path = os.path.join(output_dir, \"mvn_compile.log\")\n",
    "    stderr_path = os.path.join(output_dir, \"mvn_compile.err\")\n",
    "    run_command([\"mvn\", \"compile\"], stdout_path, stderr_path, app_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fa2e1",
   "metadata": {},
   "source": [
    "## Scanario 1:  Ehcache 2 to 3 upgrade\n",
    "\n",
    "| App         | Complexity |\n",
    "|-------------|------------|\n",
    "| Petclinic   |    High    |\n",
    "\n",
    "See description of the issue found [here](../apps/petclinic/test_cases/ehcache-2-to-3/tc.yaml).\n",
    "\n",
    "See notes on expected fix [here](../apps/petclinic/test_cases/ehcache-2-to-3/notes.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f4f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_1_name = \"ehcache-2-to-3\"\n",
    "tc_1_path = Path(\"../apps/petclinic/test_cases/ehcache-2-to-3/tc.yaml\").absolute()\n",
    "tc_1_selectors = get_test_selectors_from_tc(tc_1_path)\n",
    "repo_path = clone_app(Path(\"../apps/petclinic/app.yaml\").absolute())\n",
    "run_mvn(tc_1_name, repo_path, \"before\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
