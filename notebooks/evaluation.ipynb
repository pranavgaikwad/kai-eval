{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258fff3e",
   "metadata": {},
   "source": [
    "## LLM-as-a-Judge evaluation \n",
    "\n",
    "In this notebook, we test some approaches of evaluating responses Kai generates for different migration scenarios using an LLM as a judge.\n",
    "\n",
    "The responses are generated by running Kai manually in vscode with different models. The diff files of the responses are stored as artifacts for simplicity.\n",
    "\n",
    "#### Goal\n",
    "\n",
    "Goal of this exercise is to:\n",
    "- verify & finalize the evaluation metrics\n",
    "- finalize the evaluation prompts\n",
    "\n",
    "### Process\n",
    "\n",
    "Run Tools  -->  Run Kai (manual, responses collected)  -->  Run Tools  -->  Evaluate\n",
    "\n",
    "#### Running tools\n",
    "\n",
    "In this notebook, we run following tools before and after applying fixes from Kai:\n",
    "- `mvn compile`\n",
    "- `mvn test` (only if tests are present for a given test case)\n",
    "- `analyze` (kantra used for analysis)\n",
    "\n",
    "#### Evaluate\n",
    "\n",
    "For evaluating, three metrics are calculated:\n",
    "\n",
    "1. Completeness (C): Measures whether the issue is completely resolved\n",
    "2. Functional Parity (F): Measures whether existing functionality is maintained\n",
    "3. Knock-on Effort (E): Measures how much effort is needed to address new issues caused by the fix\n",
    "\n",
    "The total score is normalized for each metric and a final weighted score is produced between 0-10:\n",
    "\n",
    "```\n",
    "Final Score = 10 * (0.5 * C + 0.3 * F + 0.2 * E)\n",
    "```\n",
    "\n",
    "#### Pre-requisites\n",
    "\n",
    "- Create a virtualenv using Jupyter for running cells in this notebook. Use [requirements.txt](../requirements.txt) to install dependencies needed. \n",
    "- Copy the .env.sample file to .env. Select the model you want to use for evaluation - Only Bedrock and ChatOpenAI are supported. Add your LLM key for the model you want to use. Once setup, run the following cell to load .env file.\n",
    "- Make sure you have _java_ and _mvn_ installed.\n",
    "- Download the latest _kantra_ binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad69a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec81130",
   "metadata": {},
   "source": [
    "Before we begin, we write some common code in the following cell which we will need later on. Run this cell before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0849885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some common functions we will need later on for the evaluation\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import subprocess\n",
    "from git import Repo\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "test_data_path = Path(\"test-data\").absolute()\n",
    "apps_repo_path = (test_data_path / \"apps\").absolute()\n",
    "artifacts_path = (test_data_path / \"artifacts\").absolute()\n",
    "apps_repo_path.mkdir(parents=True, exist_ok=True)\n",
    "artifacts_path.mkdir(parents=True, exist_ok=True)\n",
    "## NOTE: make sure this is correct for your system\n",
    "kantra_path = Path.home() / \".kantra\" / \"kantra\"\n",
    "## NOTE: set these values\n",
    "JAVA_HOME = Path(\"/usr/lib/jvm/java-21-openjdk/\")\n",
    "JAVA_BIN = JAVA_HOME / \"bin\"\n",
    "\n",
    "def clone_repo(url: str, branch: str, path: Path):\n",
    "    try:\n",
    "        Repo.clone_from(url, depth=1, single_branch=True, branch=branch, to_path=path)\n",
    "    except Exception as e:\n",
    "        if \"already exists\" not in str(e):\n",
    "            print(\"fatal error cloning repo\")\n",
    "            sys.exit(1)\n",
    "\n",
    "def get_model():\n",
    "    provider = os.getenv(\"model_provider\")\n",
    "    model_id = os.getenv(\"model_id\")\n",
    "    if not model_id or not provider:\n",
    "        raise ValueError(\"model_id and/or model_provider are not set\")\n",
    "    match provider:\n",
    "        case \"chatbedrock\":\n",
    "            key_id = os.getenv(\"aws_access_key_id\")\n",
    "            access_key = os.getenv(\"aws_secret_access_key\")\n",
    "            region = os.getenv(\"region\")\n",
    "            if not region or not access_key or not key_id:\n",
    "                raise ValueError(\"aws_region and/or aws_secret_access_key and/or aws_access_key_id is not set\")\n",
    "            return ChatBedrockConverse(\n",
    "                model_id=model_id,\n",
    "                aws_access_key_id=key_id,\n",
    "                aws_secret_access_key=access_key,\n",
    "                region_name=region,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "        case \"chatopenai\":\n",
    "            api_key = os.getenv(\"OPEANAI_API_KEY\") or os.getenv(\"api_key\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"OPEANAI_API_KEY or api_key is not set\")\n",
    "            return ChatOpenAI(model=model_id, api_key=api_key, temperature=0.0)\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid model provider: {provider}\")\n",
    "\n",
    "def parse_yaml(path: Path): \n",
    "    with open(path, \"r\") as f: return yaml.safe_load(f)\n",
    "\n",
    "def clone_app(app_path: Path) -> Path:\n",
    "    parsed = parse_yaml(app_path)\n",
    "    repo_url = parsed[\"source_code\"][\"git\"][\"url\"]\n",
    "    branch = parsed[\"source_code\"][\"git\"][\"branch\"]\n",
    "    path = Path(\"test-data\") / \"apps\" / parsed[\"name\"]\n",
    "    clone_repo(repo_url, branch, path)\n",
    "    return path.absolute()\n",
    "\n",
    "def parse_from_tc(tc_path: str, key: str):\n",
    "    parsed = parse_yaml(tc_path)\n",
    "    return parsed[key]\n",
    "\n",
    "def run_command(cmd: list[str], stdout_path: Path, stderr_path: Path, cwd: str, env_vars: dict[str, str] = {}):\n",
    "    pwd = os.getcwd()\n",
    "    os.chdir(cwd)\n",
    "    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env_vars)\n",
    "    with open(stdout_path, \"w\") as f:\n",
    "        f.write(result.stdout)\n",
    "    with open(stderr_path, \"w\") as f:\n",
    "        f.write(result.stderr)\n",
    "    os.chdir(pwd)\n",
    "\n",
    "def apply_diff(diff_path: Path, app_path: Path):\n",
    "    repo = Repo(app_path)\n",
    "    repo.git.apply(diff_path)\n",
    "\n",
    "def git_reset(app_path: Path):\n",
    "    repo = Repo(app_path)\n",
    "    repo.git.reset(\"--hard\")\n",
    "\n",
    "def run_mvn(tc_name: str, app_path: Path, output_sub_dir: Path = Path(\"\"), env_vars: dict[str, str] = {}):\n",
    "    output_dir = (artifacts_path / tc_name / output_sub_dir / \"mvn\").absolute()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stdout_path = os.path.join(output_dir, \"mvn_compile.log\")\n",
    "    stderr_path = os.path.join(output_dir, \"mvn_compile.err\")\n",
    "    run_command([\"mvn\", \"compile\"], stdout_path, stderr_path, app_path, env_vars)\n",
    "\n",
    "def run_mvn_test(tc_name: str, app_path: Path, test_selectors: list[str], output_sub_dir: Path = Path(\"\"), env_vars: dict[str, str] = {}):\n",
    "    output_dir = (artifacts_path / tc_name / output_sub_dir / \"mvn\").absolute()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stdout_path = os.path.join(output_dir, \"mvn_test.log\")\n",
    "    stderr_path = os.path.join(output_dir, \"mvn_test.err\")\n",
    "    mvn_test_cmd = [\"mvn\", \"test\"]\n",
    "    for selector in test_selectors:\n",
    "        mvn_test_cmd.append(\"-Dtest=\")\n",
    "        mvn_test_cmd.append(selector)\n",
    "    run_command(mvn_test_cmd, stdout_path, stderr_path, app_path, env_vars)\n",
    "\n",
    "def run_kantra(tc_name: str, app_path: Path, targets: list[str], output_sub_dir: Path = Path(\"\"), env_vars: dict[str, str] = {}):\n",
    "    output_dir = (artifacts_path / tc_name / output_sub_dir / \"kantra\").absolute()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stdout_path = os.path.join(output_dir, \"kantra.log\")\n",
    "    stderr_path = os.path.join(output_dir, \"kantra.err\")\n",
    "    kantra_cmd = [kantra_path, \"analyze\", \"--overwrite\", \"--input\", app_path, \"--output\", output_dir]\n",
    "    for target in targets:\n",
    "        kantra_cmd.append(\"--target\")\n",
    "        kantra_cmd.append(target.strip('\"'))\n",
    "    env_vars = os.environ.copy()\n",
    "    env_vars[\"JAVA_HOME\"] = JAVA_HOME\n",
    "    env_vars[\"PATH\"] = os.environ[\"PATH\"] + \":\" + str(JAVA_BIN)\n",
    "    run_command(kantra_cmd, stdout_path, stderr_path, app_path, env_vars)\n",
    "\n",
    "METRICS = {\n",
    "    \"TRIVIAL_CHANGES_NEEDED\": 1,\n",
    "    \"COMPLEX_CHANGES_NEEDED\": 3,\n",
    "    \"REDESIGN_NEEDED\": 9,\n",
    "    \"NOT_FIXED\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06761bb",
   "metadata": {},
   "source": [
    "In the following cell, we have our LLM-as-a-Judge code. Run this cell before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbd38dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from git import Repo, Head\n",
    "from pathlib import Path\n",
    "from typing import Annotated\n",
    "from dataclasses import dataclass, field\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "@dataclass\n",
    "class EvaluationTools:\n",
    "    app_path: Path\n",
    "    repo_path: Path \n",
    "    tc_path: Path\n",
    "    diff_path: Path\n",
    "    changed_filepaths: list[str] = field(default_factory=list)\n",
    "    changed_filelist: list[str] = field(default_factory=list)\n",
    "    changed_files: list[str] = field(default_factory=dict)\n",
    "    architecture_md: str = \"Not available\"\n",
    "\n",
    "    def init(self):\n",
    "        self.architecture_md = (self.app_path / \"architecture.md\").read_text()\n",
    "        self.repo = Repo(self.repo_path)\n",
    "        self.repo.git.reset(\"--hard\")\n",
    "        self.repo.git.apply(self.diff_path)\n",
    "        self.changed_filepaths = self.repo.git.diff(\"--name-only\").split()\n",
    "        self.changed_filelist = [str(p.relative_to(self.repo_path)) for p in Path(self.repo_path).rglob(\"*\") if p.is_file()]\n",
    "        for path in self.changed_filepaths:\n",
    "            self.changed_files[path] = (self.repo_path / Path(path)).read_text()\n",
    "        self.repo.git.reset(\"--hard\")\n",
    "\n",
    "    def get_architecture_tool(self):\n",
    "        architecture_md = self.architecture_md\n",
    "        @tool\n",
    "        def get_architecture():\n",
    "            \"\"\"Returns the architecture of the application in markdown format\"\"\"\n",
    "            return architecture_md\n",
    "        return get_architecture\n",
    "    \n",
    "    def get_changed_files_tool(self):\n",
    "        changed_filepaths = self.changed_filepaths\n",
    "        @tool\n",
    "        def get_changed_files():\n",
    "            \"\"\"Returns the list of files that have been changed after applying the fixes\"\"\"\n",
    "            return \"\\n\".join(changed_filepaths)\n",
    "        return get_changed_files\n",
    "    \n",
    "    def get_file_content_tool(self):\n",
    "        changed_files = self.changed_files\n",
    "        repo_path = self.repo_path\n",
    "        @tool\n",
    "        def get_file_content(\n",
    "            file_path: Annotated[str, \"Relative path to the file\"],\n",
    "            pre_migration: Annotated[bool, \"Return a pre-migration version, defaults to False\"] = False):\n",
    "            \"\"\"Returns the current content of a file, if pre_migration is set, returns its old content\"\"\"\n",
    "            if pre_migration or file_path not in self.changed_files:\n",
    "                return (repo_path / file_path).read_text()\n",
    "            return changed_files[file_path]\n",
    "        return get_file_content\n",
    "\n",
    "    def list_files_tool(self):\n",
    "        changed_filelist = self.changed_filelist\n",
    "        repo_path = self.repo_path\n",
    "        @tool\n",
    "        def list_files(\n",
    "            pre_migration: Annotated[bool, \"Set to True to return pre-migration list of files, defaults to False\"] = False):\n",
    "            \"\"\"Returns a list of files in the source code\"\"\"\n",
    "            if pre_migration:\n",
    "                return \"\\n\".join([str(p.relative_to(repo_path)) for p in Path(repo_path).rglob(\"*\") if p.is_file()])\n",
    "            return \"\\n\".join(changed_filelist)\n",
    "        return list_files\n",
    "\n",
    "def run_completeness_agent(\n",
    "    issue: str,\n",
    "    issue_notes: str,\n",
    "    source_tech: str,\n",
    "    target_tech: str,\n",
    "    app_path: Path,\n",
    "    repo_path: Path,\n",
    "    tc_path: Path,\n",
    "    diff_path: Path,\n",
    ") -> int:\n",
    "    COMPLETENESS_PROMPTS = {\n",
    "    \"system\": \"\"\"You are a senior software engineer expert in migrating applications from one technology to another.\"\"\",\n",
    "    \"user_summarize\": \"\"\"You are reviewing a code change made to fix a migration issue identified by a static analysis tool.\n",
    "The application is being migrated from {source_tech} to {target_tech}.\n",
    "You are provided with:\n",
    "- The original issue description\n",
    "- Developer notes describing the intended fix (consider these the only source of truth)\n",
    "- Access to a set of tools that can inspect the codebase\n",
    "\n",
    "## Your task\n",
    "\n",
    "Your goal is to evaluate whether the migration issue has been completely fixed by comparing:\n",
    "- What the notes say should have been done, and\n",
    "- What was actually done in the codebase.\n",
    "\n",
    "## Evaluation Rules\n",
    "- Stick strictly to the notes when comparing. They are the authoritative reference.\n",
    "- Do not assume additional requirements, behaviors, or conventions.\n",
    "- Use tools thoughtfully. Only gather information necessary to confirm alignment between the notes and the actual code changes.\n",
    "- Be specific. When identifying missing or incomplete work, clearly describe what evidence led you to that conclusion.\n",
    "- Be concise but thorough. Focus on correctness and completeness.\n",
    "- Refer explicitly to evidence from the codebase when making your assessment.\n",
    "- Avoid subjective language like \"it seems\" or \"probably\".\n",
    "\n",
    "## Output Format\n",
    "\n",
    "If the issue is completely fixed, respond in the following format:\n",
    "\n",
    "```\n",
    "<Brief summary explaining how the applied changes match the notes and why they fully resolve the issue.>\n",
    "\n",
    "COMPLETELY_FIXED\n",
    "```\n",
    "\n",
    "If the issue is not completely fixed, respond in the following format:\n",
    "\n",
    "```\n",
    "<Brief summary explaining why the fix is incomplete or incorrect, and what parts are missing or deviate from the notes.>\n",
    "\n",
    "## Issues\n",
    "\n",
    "1. <First issue summary>\n",
    "2. <Second issue summary>\n",
    "...\n",
    "```\n",
    "\n",
    "Each issue should represent a *logically distinct* missing or incorrect aspect of the fix. Do not produce duplicate issues. Group related issues together logically.\n",
    "\n",
    "Here are your inputs:\n",
    "## The issue identified was:\n",
    "{issue}\n",
    "\n",
    "## Here are the notes about the issue:\n",
    "{issue_notes}\n",
    "\"\"\",\n",
    "    \"user_rate\": \"\"\"We used a static analysis tool to identify a migration issue in the application which needs to be migrated from {source_tech} to {target_tech}.\n",
    "We fixed the issue and asked a senior software engineer to review the changes. The engineer reviewed the changes and provided a summary of the changes made.\n",
    "\n",
    "## Your task\n",
    "\n",
    "Your goal is to:\n",
    "\n",
    "- Analyze each *distinct unresolved issue* described in the summary.\n",
    "- Assess the complexity of fixing each issue for given application.\n",
    "- Finally, provide a rating for \"completeness\" of the fix based on the following scale:\n",
    "    - TRIVIAL_CHANGES_NEEDED: Requires only a few small, localized modifications to files in the codebase (e.g., renaming, updating a parameter, minor logic tweak, or adding a missing import).\n",
    "    - COMPLEX_CHANGES_NEEDED: Requires multiple related changes across files, complex logic changes, or coordination between components or services.\n",
    "    - REDESIGN_NEEDED: Indicates a fundamental design or architectural flaw requiring a significant refactor or reimplementation of core logic.\n",
    "    - COMPLETE: No issues remain; the migration fix is fully complete.\n",
    "\n",
    "## Guidelines\n",
    "- Focus only on issues explicitly described in the summary.\n",
    "- Do not invent or infer new issues beyond what is stated.\n",
    "- Use architectural reasoning (e.g., cross-file dependencies, data flow, API contracts) to decide the appropriate rating.\n",
    "- Each issue should have a short justification describing why it falls into that category.\n",
    "- Maintain objectivity — avoid vague or subjective language like “probably complex” or “seems fine.”\n",
    "- If there are no issues identified in the summary, rate the migration as COMPLETE.\n",
    "- Look at architecture of the application to help you make the decision.\n",
    "\n",
    "Produce your response in following format:\n",
    "\n",
    "```\n",
    "<Brief reasoning behind the rating.>\n",
    "\n",
    "Rating: <RATING>\n",
    "```\n",
    "\n",
    "Here are your inputs:\n",
    "\n",
    "## Issue we fixed\n",
    "{issue}\n",
    "\n",
    "## Summary\n",
    "{summary}\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "    tool_factory = EvaluationTools(app_path, repo_path, tc_path, diff_path)\n",
    "    tool_factory.init()\n",
    "    tools = [\n",
    "        tool_factory.get_architecture_tool(),\n",
    "        tool_factory.get_changed_files_tool(),\n",
    "        tool_factory.get_file_content_tool(),\n",
    "    ]\n",
    "    agent = create_react_agent(\n",
    "        model=get_model(),\n",
    "        tools=tools,\n",
    "        prompt=COMPLETENESS_PROMPTS[\"system\"],\n",
    "    )\n",
    "    response = agent.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": COMPLETENESS_PROMPTS[\"user_summarize\"].format(\n",
    "                        source_tech=source_tech,\n",
    "                        target_tech=target_tech,\n",
    "                        issue=issue,\n",
    "                        issue_notes=issue_notes,\n",
    "                    )\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    rate_agent = create_react_agent(\n",
    "        model=get_model(),\n",
    "        tools=[tool_factory.get_architecture_tool()],\n",
    "        prompt=COMPLETENESS_PROMPTS[\"system\"],\n",
    "    )\n",
    "    response = rate_agent.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": COMPLETENESS_PROMPTS[\"user_rate\"].format(\n",
    "                        source_tech=source_tech,\n",
    "                        target_tech=target_tech,\n",
    "                        issue=issue,\n",
    "                        summary=response[\"messages\"][-1].content,\n",
    "                    )\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    content = response[\"messages\"][-1].content\n",
    "    match = re.search(r\"Rating:\\s*([^\\n\\r]+)\", content)\n",
    "    rating = -1\n",
    "    if match:\n",
    "        try:\n",
    "            val = int(METRICS.get(match.group(1).strip(), -1))\n",
    "            if val == -1:\n",
    "                return val\n",
    "            rating = (max(METRICS.values()) - int(val)) / max(METRICS.values())\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return rating\n",
    "\n",
    "def run_functional_correctness_agent(\n",
    "    issue: str,\n",
    "    issue_notes: str,\n",
    "    source_tech: str,\n",
    "    target_tech: str,\n",
    "    repo_path: Path,\n",
    "    tc_path: Path,\n",
    "    diff_path: Path,\n",
    "):\n",
    "    FUNCTIONAL_CORRECTNESS_PROMPTS = {\n",
    "        \"system\": \"You are a senior engineering expert in migrating source code as well as reviewing source code that is migrated from {source_tech} to {target_tech}.\",\n",
    "        \"user_summarize\": \"\"\"You are evaluating whether the migrated code behaves functionally equivalent to its pre-migration behavior and/or meets the acceptance criteria defined for the migration.\n",
    "\n",
    "You are given:\n",
    "- The original migration issue that was intended to be fixed.\n",
    "- Detailed notes describing the expected fix and acceptance criteria (this is the sole source of truth).\n",
    "- Various tools to access post migration artifacts such as changed files, contents of pre and post migration files, results of behavioral tests (if present) and more.\n",
    "\n",
    "## Your task\n",
    "\n",
    "Determine whether the migrated code preserves the functional behavior of the pre-migration code.\n",
    "Do not rely on assumptions, intuition, or unstated expectations.\n",
    "Follow a deterministic and evidence-driven process:\n",
    "\n",
    "- Examine pre-migration code to identify control flow, data flow, API contracts, input constraints, and edge-case handling.\n",
    "- Refer to architecture.md (if available) to understand context and dependencies.\n",
    "- Use the migration notes as the only authoritative specification for what the fix should achieve.\n",
    "- Do not invent new requirements or infer functionality not explicitly mentioned.\n",
    "- For each described behavior, check that the migrated version maintains equivalent logic, inputs/outputs, and side effects.\n",
    "- Pay attention to API signatures, data transformations, validation rules, and error handling.\n",
    "- Use behavioral tests to understand the expected runtime behavior.\n",
    "- If tests fail, determine why:\n",
    "  - If due to test harness or environment adaptation (e.g., dependency mismatch, configuration differences), this does not count against equivalence.\n",
    "  - If due to missing or altered functionality, consider this a deviation.\n",
    "- You are assessing functional parity, not quality, maintainability, or completeness of the migration itself.\n",
    "- Your conclusion should be based solely on behavior and conformance to the notes.\n",
    "\n",
    "## Your output\n",
    "\n",
    "Finally, rate \"functional equivalence\" on a scale defined below:\n",
    "* EQUIVALENT\n",
    "  - The migrated code preserves all functional behavior compared to its pre-migration version.\n",
    "  - Any test failures are attributable only to the harness or environment, not to business logic or data flow differences.\n",
    "* SOMEWHAT_EQUIVALENT\n",
    "  - The core behavior is largely preserved, but minor functional gaps exist that could be fixed with small code tweaks.\n",
    "  - Tests fail due to small missing pieces or outdated expectations, but the intended behavior remains intact.\n",
    "* NOT_EQUIVALENT\n",
    "  - Critical functionality altered / missing in the migrated version of the code.\n",
    "  - Behavioral expectations in the notes or pre-migration logic are not met.\n",
    "  - Test failures reflect true functional regressions, not harness issues.\n",
    "\n",
    "Produce your output in format below:\n",
    "\n",
    "```\n",
    "<Brief, objective reasoning for your rating — reference specific files, functions, or behaviors as evidence.>\n",
    "\n",
    "Rating: <EQUIVALENT | SOMEWHAT_EQUIVALENT | NOT_EQUIVALENT>\n",
    "```\n",
    "\n",
    "Here are your inputs:\n",
    "\n",
    "## Original issue\n",
    "\n",
    "{issue}\n",
    "\n",
    "## Issue notes\n",
    "\n",
    "{issue_notes}\n",
    "\n",
    "## Applicable behvioral tests\n",
    "{}\n",
    "\"\"\",\n",
    "    }\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fa2e1",
   "metadata": {},
   "source": [
    "## Scanario 1:  Ehcache 2 to 3 upgrade\n",
    "\n",
    "| App         | Complexity |\n",
    "|-------------|------------|\n",
    "| Petclinic   |    High    |\n",
    "\n",
    "See description of the issue found [here](../apps/petclinic/test_cases/ehcache-2-to-3/tc.yaml).\n",
    "\n",
    "See notes on expected fix [here](../apps/petclinic/test_cases/ehcache-2-to-3/notes.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f4f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tools -> apply diff -> run tools\n",
    "\n",
    "tc_1_name = \"ehcache-2-to-3\"\n",
    "tc_1_app_path = Path(\"../apps/petclinic/app.yaml\").absolute()\n",
    "tc_1_path = Path(\"../apps/petclinic/test_cases/ehcache-2-to-3/tc.yaml\").absolute()\n",
    "tc_1_selectors = parse_from_tc(tc_1_path, \"testSelectors\")\n",
    "tc_1_targets = parse_from_tc(tc_1_app_path, \"targets\")\n",
    "tc_1_repo_path = clone_app(tc_1_app_path)\n",
    "run_mvn(tc_1_name, tc_1_repo_path, \"before\")\n",
    "run_mvn_test(tc_1_name, tc_1_repo_path, tc_1_selectors, \"before\")\n",
    "run_kantra(tc_1_name, tc_1_repo_path, tc_1_targets, \"before\")\n",
    "\n",
    "tc_1_agent_diff = Path(\"test-data/diffs/ehcache-2-to-3/gpt_4o_agent.diff\").absolute()\n",
    "tc_1_non_agent_diff = Path(\"test-data/diffs/ehcache-2-to-3/gpt_4o_non_agent.diff\").absolute()\n",
    "apply_diff(tc_1_agent_diff, tc_1_repo_path)\n",
    "run_mvn(tc_1_name, tc_1_repo_path, \"after\")\n",
    "run_mvn_test(tc_1_name, tc_1_repo_path, tc_1_selectors, \"after\")\n",
    "run_kantra(tc_1_name, tc_1_repo_path, tc_1_targets, \"after\")\n",
    "git_reset(tc_1_repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0a467",
   "metadata": {},
   "source": [
    "Now we have all the data we need for pre and post fix. We will use an LLM to evaluate the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd8e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness score:  0.9\n"
     ]
    }
   ],
   "source": [
    "## Evaluating for completeness metric\n",
    "tc_1_notes = (tc_1_path.parent / \"notes.md\").read_text()\n",
    "tc_1_issue = parse_from_tc(tc_1_path, \"description\")\n",
    "print(\"Completeness score: \", \n",
    "    run_completeness_agent(tc_1_issue, tc_1_notes, \n",
    "    \"Spring Framework 5\", \"Spring Framework 6\", \n",
    "    tc_1_app_path.parent, tc_1_repo_path, tc_1_path, tc_1_agent_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52cf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
